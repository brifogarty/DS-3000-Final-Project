{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawler: [spotifycharts.com](https://spotifycharts.com/)\n",
    "## Contents:\n",
    "**0. [Imports](#0.-Imports)**  \n",
    "**1. [Helper Methods](#1.-Helper-Methods)**  \n",
    "**2. [Chart Crawling and Scraping Methods](#2.-Chart-Crawling-and-Scraping-Methods)**  \n",
    "**3. [Method Call](#3.-Method-Call)**  \n",
    "**4. [Testing](#4.-Testing)**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import http.client\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import dateutil.parser\n",
    "import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_trend_type(trend_fill:str) -> str:\n",
    "    \"\"\"Returns the trend type given the fill color of the Spotify Chart trend SVG.\"\"\"\n",
    "    # i. color: green; shape: up-triangle\n",
    "    if trend_fill == '#84bd00':\n",
    "        return 'Up'\n",
    "    # ii. color: gray; shape: horizontal-rectangle\n",
    "    elif trend_fill == '#3e3e40':\n",
    "        return 'Flat'\n",
    "    # iii. color: red; shape: down-triangle\n",
    "    elif trend_fill == '#bd3200':\n",
    "        return 'Down'\n",
    "    # iv. color: blue; shape: circle\n",
    "    elif trend_fill == '#4687d7':\n",
    "        return 'New'\n",
    "    # v. unhandled: non-v.1.1 tend_fill\n",
    "    else:\n",
    "        return 'UNKNOWN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datestamp_str(date):\n",
    "    \"\"\"Returns the current datestamp as a string in YY-MM-DD format.\"\"\"\n",
    "    datestamp_str = date.strftime('%Y-%m-%d')\n",
    "    return datestamp_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp_str():\n",
    "    \"\"\"Returns the current timestamp as a string in YY-MM-DD HH-SS-FF format.\"\"\"\n",
    "    now_datetime = datetime.datetime.now()\n",
    "    timestamp_str = now_datetime.strftime('D-%Y-%m-%d_T-%H-%M-%S-%f')\n",
    "    return timestamp_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    \"\"\"Returns the BeautifulSoup object for the given URL.\"\"\"\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'\n",
    "    url_req = urllib.request.Request(url, headers = {'User-Agent': user_agent})\n",
    "    req_resp = urllib.request.urlopen(url_req)\n",
    "    req_read = req_resp.read()\n",
    "    req_resp.close()\n",
    "    return BeautifulSoup(req_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_region_dict(chart_type_url):\n",
    "    \"\"\"Returns a dictionary of region abbreviation keys and region name values.\"\"\"\n",
    "    chart_soup = get_soup(chart_type_url)\n",
    "    region_li_tags = chart_soup.find('div', \n",
    "                                     attrs={'class':'responsive-select','data-type':'country'}).find_all('li')\n",
    "    region_dict = {}\n",
    "    for li_tag in region_li_tags:\n",
    "        region_abrv = li_tag.get('data-value').strip()\n",
    "        region_name = li_tag.get_text()\n",
    "        region_dict[region_abrv] = region_name\n",
    "    \n",
    "    return region_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_charts_dates(region_url, strt_date_arg, end_date_arg):\n",
    "    \"\"\"Returns a list of date strings representing the available charts.\"\"\"\n",
    "    region_soup = get_soup(region_url)\n",
    "    date_div = region_soup.find('div', attrs={'class':'responsive-select','data-type':'date'})\n",
    "    \n",
    "    if date_div is None:\n",
    "        return np.array([], dtype=datetime.date)\n",
    "    \n",
    "    else:\n",
    "        date_li_tags = date_div.find_all('li')\n",
    "        \n",
    "        first_li_tag = date_li_tags[0]\n",
    "        first_li_date = dateutil.parser.isoparse(first_li_tag.get('data-value').strip()).date()\n",
    "        max_date = first_li_date if end_date_arg is None else end_date_arg\n",
    "\n",
    "        last_li_tag = date_li_tags[-1]\n",
    "        last_li_date = dateutil.parser.isoparse(last_li_tag.get('data-value').strip()).date()\n",
    "        min_date = last_li_date if strt_date_arg is None else strt_date_arg\n",
    "        \n",
    "        num_days_delta = (max_date - min_date).days\n",
    "        date_range_list = [min_date + datetime.timedelta(days=x) for x in range(num_days_delta + 1)]\n",
    "    \n",
    "        return date_range_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_error(chart_date, chart_region_abrv, chart_name, idx , chart_date_url, error_code):\n",
    "    \"\"\"Returns a list of and out prints error details.\"\"\"\n",
    "    print('ERROR  : ',  'attempt ', idx, ' ; ', error_code)\n",
    "    print('         ', chart_date, ' - ', chart_region_abrv, ' - ', chart_name, ' - ', chart_date_url)\n",
    "    return [chart_name, chart_region_abrv, chart_date, chart_date_url, idx, str(error_code)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_csv_filepath(type_str, now_timestamp, start_datestamp, end_datestamp):\n",
    "    dir_path = './data/'+now_timestamp\n",
    "    if not os.path.exists(dir_path): os.mkdir(dir_path)\n",
    "    \n",
    "    file_name = type_str + now_timestamp + '_FrmD-' + start_datestamp + '_ToD-' + end_datestamp + '.csv'\n",
    "    file_path = os.path.join(dir_path, file_name)\n",
    "    \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_analysis_report(result_df_arg, error_df_arg):\n",
    "    \"\"\"Prints analysis metrics for the chart crawl.\"\"\"\n",
    "    print('\\nRESULT ANALYSIS')\n",
    "    \n",
    "    result_analysis_dict = {\n",
    "        'Num Rows' : result_df_arg.shape[0],\n",
    "        'Num Cols' : result_df_arg.shape[1],\n",
    "        'Num Unique Dates' : len(result_df_arg['Date'].unique()),\n",
    "        'Num Unique Charts' : len(result_df_arg['Chart'].unique()),\n",
    "        'Num Unique Regions' : len(result_df_arg['Region'].unique()),\n",
    "        'Num Unique Positions' : len(result_df_arg['Position'].unique()),\n",
    "        'Num Unique Trends' : len(result_df_arg['Trend'].unique()),\n",
    "        'Num Unique Titles' : len(result_df_arg['Title'].unique()),\n",
    "        'Num Unique Artists' : len(result_df_arg['Artist'].unique()),\n",
    "        'Num Unique Icon_URLs' : len(result_df_arg['Icon_URL'].unique()),\n",
    "        'Num Unique Spotify_URLs' : len(result_df_arg['Spotify_URL'].unique())\n",
    "    }\n",
    "    \n",
    "    result_analysis_df = pd.DataFrame(data=list(result_analysis_dict.values()), columns=['Value'])\n",
    "    result_analysis_df['Metric'] = list(result_analysis_dict.keys())\n",
    "    result_analysis_df.set_index('Metric', inplace=True)\n",
    "    \n",
    "    rows_equals_dates = (result_analysis_dict['Num Rows'] \n",
    "                         == result_analysis_dict['Num Unique Dates']*200)\n",
    "    titles_equals_urls = (result_analysis_dict['Num Unique Titles'] \n",
    "                          == result_analysis_dict['Num Unique Spotify_URLs'])\n",
    "    \n",
    "    min_date = result_df_arg['Date'].min()\n",
    "    max_date = result_df_arg['Date'].max()\n",
    "    delta_days = (max_date - min_date).days\n",
    "    date_equals_delta = (len(result_df_arg['Date'].unique()) == delta_days)\n",
    "\n",
    "    print('\\n-----\\n')\n",
    "    print('RESULTS:\\n')\n",
    "    print(result_analysis_df)\n",
    "    print('\\nNum Rows==Dates?')\n",
    "    print(' – YES' if rows_equals_dates else ' – NO: ' + str(result_analysis_dict['Num Rows']) \n",
    "          + ' ≠ ' + str(result_analysis_dict['Num Unique Dates']*200) \n",
    "          + ' = ' + str(result_analysis_dict['Num Unique Dates']) + ' * 200')\n",
    "    print('\\nNum Titles==Spotify_URLs?')\n",
    "    print(' – YES' if titles_equals_urls else ' – NO: '+ str(result_analysis_dict['Num Unique Titles']) \n",
    "          + ' ≠ ' + str(result_analysis_dict['Num Unique Spotify_URLs']))\n",
    "    print('\\nNum Dates==Date Delta?')\n",
    "    print(' – YES' if date_equals_delta else ' – NO: '+ str(len(result_df_arg['Date'].unique())) \n",
    "          + ' ≠ ' + str(delta_days))\n",
    "\n",
    "    \n",
    "    # Error Col Names:\n",
    "    # 'Chart', 'Region', 'Date', 'Chart URL', 'Attempt', 'Error'\n",
    "    \n",
    "    incomp_read_df = error_df_arg[error_df_arg['Error'].str.startswith('IncompleteRead')]\n",
    "    http_df = error_df_arg[error_df_arg['Error'].str.startswith('HTTP Error')]\n",
    "    skipped_df = error_df_arg[error_df_arg['Attempt']==-1]\n",
    "    combined_df = (error_df_arg['Error'].str.startswith('IncompleteRead', na=True) | \n",
    "                   error_df_arg['Error'].str.startswith('HTTP Error', na=True))\n",
    "    \n",
    "    error_analysis_dict = {\n",
    "        'Num IncompleteRead' : len(incomp_read_df),\n",
    "        'Num HTTP' : len(http_df),\n",
    "        'Num Other' : combined_df[combined_df==False].count(),\n",
    "        'Num Total Errors' : len(error_df_arg),\n",
    "    }\n",
    "    \n",
    "    error_analysis_df = pd.DataFrame(data=list(error_analysis_dict.values()), columns=['Value'])\n",
    "    error_analysis_df['Metric'] = list(error_analysis_dict.keys())\n",
    "    error_analysis_df.set_index('Metric', inplace=True)\n",
    "    \n",
    "    http_equals_skips = (error_analysis_dict['Num HTTP'] == len(skipped_df))\n",
    "    \n",
    "    error_sum = (error_analysis_dict['Num IncompleteRead'] \n",
    "                 + error_analysis_dict['Num HTTP'] \n",
    "                 + error_analysis_dict['Num Other'])\n",
    "    \n",
    "    total_equals_sum = error_analysis_dict['Num Total Errors'] == error_sum\n",
    "    \n",
    "    print('\\n-----\\n')\n",
    "    print('ERRORS:\\n')\n",
    "    print(error_analysis_df)\n",
    "    print('\\nNum HTTP==Skips?')\n",
    "    print(' – YES' if http_equals_skips else ' – NO: ' + str(error_analysis_dict['Num HTTP']) \n",
    "          + ' ≠ ' + str(len(skipped_df)))\n",
    "    print('\\nNum Total==Sum?')\n",
    "    print(' – YES' if total_equals_sum else ' – NO: '+ str(error_analysis_dict['Num Total Errors']) \n",
    "          + ' ≠ ' + str(error_sum)\n",
    "          + ' = (' + str(error_analysis_dict['Num IncompleteRead']) \n",
    "          + ' + ' + str(error_analysis_dict['Num HTTP']) \n",
    "          + ' + ' + str(error_analysis_dict['Num Other']) + ' )')\n",
    "    print('\\nMin date:', min_date)\n",
    "    print('Max date:', max_date)\n",
    "    print('\\nSKIPPED CHARTS:\\n')\n",
    "    print('\\n')\n",
    "    print(skipped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Chart Crawling and Scraping Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_chart(date_soup, chart_name, chart_region, chart_date):\n",
    "    \"\"\"Returns pd/DataFrame of data scraped from each chart URL's <table class=\"chart-table\" ...> tag\"\"\"\n",
    "    tr_tags = date_soup.find('table', class_='chart-table').find('tbody').find_all('tr')\n",
    "    chart_data = []\n",
    "    \n",
    "    for tr_tag in tr_tags:\n",
    "        \n",
    "        song_info = []\n",
    "        \n",
    "        # 0. col1, col2, col3: Chart Name, Chart Region, and Chart Date\n",
    "        song_info.append(chart_name)\n",
    "        song_info.append(chart_region)\n",
    "        song_info.append(chart_date)\n",
    "        \n",
    "        # 1. col4: Chart Position; element of {1, 2, ... , 200}\n",
    "        pos_tag = tr_tag.find('td', class_='chart-table-position')\n",
    "        pos_val = int(pos_tag.get_text().strip())\n",
    "        song_info.append(pos_val)\n",
    "        \n",
    "        # 2. col5: Streaming Trend Type; element of {Up, Flat, Down, New}\n",
    "        trend_tag = tr_tag.find('td', class_='chart-table-trend')\n",
    "        trend_fill = trend_tag.find('svg').get('fill').strip()\n",
    "        trend_val = det_trend_type(trend_fill)\n",
    "        song_info.append(trend_val)\n",
    "        \n",
    "        # 3. col6, col7: Track Title and Artist\n",
    "        track_tag = tr_tag.find('td', class_='chart-table-track')\n",
    "        title_val = track_tag.find('strong').get_text().strip()\n",
    "        artist_val = track_tag.find('span').get_text().strip().replace('by ', '')\n",
    "        song_info.append(title_val)\n",
    "        song_info.append(artist_val)\n",
    "        \n",
    "        # 4. col8: Total Daily Streams\n",
    "        if chart_name == 'Viral50':\n",
    "            song_info.append(None)\n",
    "        else:\n",
    "            streams_tag = tr_tag.find('td', class_='chart-table-streams')\n",
    "            streams_val = int(streams_tag.get_text().strip().replace(',',''))\n",
    "            song_info.append(streams_val)\n",
    "        \n",
    "        # 5. col9, col10: Icon and Spotify URLs\n",
    "        icon_tag = tr_tag.find('td', class_='chart-table-image')\n",
    "        icon_url_val = icon_tag.find('img').get('src').strip()\n",
    "        spotify_url_val = icon_tag.find('a').get('href').strip()\n",
    "        song_info.append(icon_url_val)\n",
    "        song_info.append(spotify_url_val)\n",
    "        \n",
    "        #6. col11: Spotify ID\n",
    "        spotify_id_val = spotify_url_val[(spotify_url_val.rfind('/') + 1):].strip()\n",
    "        song_info.append(spotify_id_val)\n",
    "        \n",
    "        chart_data.append(song_info)\n",
    "\n",
    "    return chart_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_spotify_charts(start_date=None,\n",
    "                         end_date=None,\n",
    "                         select_regions_dict=None,\n",
    "                         crawl_viral=False,\n",
    "                         crawl_weekly=False,\n",
    "                         colab_save=False):\n",
    "    \"\"\"Returns a pd.DataFrame containing information scraped from https://spotifycharts.com.\"\"\"\n",
    "    # 1. loop through charts\n",
    "    base_url = 'https://spotifycharts.com'\n",
    "    \n",
    "    chart_type_dict = {'regional':'Top200', 'viral':'Viral50'} if crawl_viral else {'regional':'Top200'}\n",
    "    chart_interval_list = ['daily', 'weekly'] if crawl_weekly else ['daily']\n",
    "    \n",
    "    # 2. instantiates result lists\n",
    "    result_data = []\n",
    "    error_list = []\n",
    "    \n",
    "    print('----------------------------------------------------------------------')\n",
    "    print('\\nWEB CRAWLING LOG:\\n\\n')\n",
    "\n",
    "    # 3. scrapes all available charts\n",
    "    # 3.1. loops through chart types\n",
    "    for chart_type, chart_name in chart_type_dict.items():\n",
    "        \n",
    "        chart_type_url = base_url + '/' + chart_type\n",
    "        if select_regions_dict is None:\n",
    "            chart_regions_dict = scrape_region_dict(chart_type_url)\n",
    "        else:\n",
    "            chart_regions_dict = select_regions_dict\n",
    "\n",
    "        # 3.2. loops through chart regions\n",
    "        for chart_region_abrv, chart_region_name in chart_regions_dict.items():\n",
    "\n",
    "            # 3.3. loops through chart intervals\n",
    "            for chart_interval in chart_interval_list:\n",
    "                \n",
    "                latest_charts_url = chart_type_url + '/' + chart_region_abrv + '/' + chart_interval\n",
    "\n",
    "                try:\n",
    "                    charts_dates_list = scrape_charts_dates(latest_charts_url, start_date, end_date)\n",
    "                    \n",
    "                except (http.client.IncompleteRead, urllib.error.HTTPError) as err:\n",
    "                    error_list.append(handle_error(chart_date, chart_region_abrv, \n",
    "                                                   chart_name, np.nan , chart_date_url, err))\n",
    "                    continue\n",
    "                \n",
    "                # 3.4. loops through all available chart dates\n",
    "                for chart_date in charts_dates_list:\n",
    "\n",
    "                    chart_date_url = latest_charts_url + '/' + chart_date.isoformat()\n",
    "\n",
    "                    # 3.4.1. tries, then retries 3 times, to read the chart's html page\n",
    "                    for attempt in range(1,4):\n",
    "                        # 3.4.1.1. tries to read the url, and if successful, scrapes and appends its data\n",
    "                        try:\n",
    "                            chart_date_soup = get_soup(chart_date_url)\n",
    "                            chart_date_data = scrape_chart(chart_date_soup, \n",
    "                                                           chart_name, \n",
    "                                                           chart_regions_dict[chart_region_abrv], \n",
    "                                                           chart_date)\n",
    "                            result_data.append(chart_date_data)\n",
    "                            print('Success: ', chart_date, \n",
    "                                  ' - ', chart_region_abrv, \n",
    "                                  ' - ', chart_name, \n",
    "                                  ' - ', chart_date_url)\n",
    "                            break\n",
    "\n",
    "                        # 3.4.1.2. cathces IncompleteRead exceptions, retrying the URL twice\n",
    "                        except (http.client.IncompleteRead, urllib.error.HTTPError) as err:\n",
    "                            error_list.append(handle_error(chart_date, chart_region_abrv, \n",
    "                                                           chart_name, attempt , chart_date_url, err))\n",
    "                            continue\n",
    "\n",
    "                    # 3.5. otherwise, if the loop falls through, skips this chart date\n",
    "                    else:\n",
    "                        error_list.append(handle_error(chart_date, chart_region_abrv, \n",
    "                                                       chart_name, -1 , chart_date_url, 'SKIPPED'))\n",
    "                        continue\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------')\n",
    "    print('\\n\\tCOMPLETED: WEB CRAWLING/SCRAPING')\n",
    "                        \n",
    "    # 4. creates both the result and error pd.DataFrames from the accumulated list of data entries\n",
    "    result_data_flat = [chart_entry for date_data in result_data for chart_entry in date_data]\n",
    "    result_df_cols = ['Chart', 'Region' , 'Date', 'Position', 'Trend', \n",
    "                      'Title', 'Artist', 'Streams', 'Icon_URL', 'Spotify_URL', 'Spotify_ID']\n",
    "    result_df = pd.DataFrame(result_data_flat, columns=result_df_cols)\n",
    "    \n",
    "    error_df_cols = ['Chart', 'Region', 'Date', 'Chart URL', 'Attempt', 'Error']\n",
    "    error_df = pd.DataFrame(error_list, columns=error_df_cols)                 \n",
    "    \n",
    "    # 5. saves both the result and error pd.DataFrame as csv files  \n",
    "    now_timestamp = get_timestamp_str()\n",
    "    start_datestamp = get_datestamp_str(charts_dates_list[0])\n",
    "    end_datestamp = get_datestamp_str(charts_dates_list[-1])\n",
    "    \n",
    "    result_df.to_csv(gen_csv_filepath('Result_', now_timestamp, start_datestamp, end_datestamp))\n",
    "    \n",
    "    top200_df = result_df[result_df['Chart']=='Top200']\n",
    "    top200_df.to_csv(gen_csv_filepath('Top200_', now_timestamp, start_datestamp, end_datestamp))\n",
    "    \n",
    "    viral50_df = result_df[result_df['Chart']=='Viral50']\n",
    "    viral50_df.to_csv(gen_csv_filepath('Viral50_', now_timestamp, start_datestamp, end_datestamp))\n",
    "    \n",
    "    error_df.to_csv(gen_csv_filepath('Errors_', now_timestamp, start_datestamp, end_datestamp))\n",
    "    \n",
    "    print('\\n\\tCOMPLETED: DATAFRAME/CSV CREATION')\n",
    "    print('\\n----------------------------------------------------------------------')\n",
    "    gen_analysis_report(result_df, error_df)\n",
    "    print('\\n----------------------------------------------------------------------')\n",
    "    \n",
    "    # 6. returns the result pd.DataFrame\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Method Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "\n",
      "WEB CRAWLING LOG:\n",
      "\n",
      "\n",
      "Success:  2017-01-01  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-01\n",
      "Success:  2017-01-02  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-02\n",
      "Success:  2017-01-03  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-03\n",
      "Success:  2017-01-04  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-04\n",
      "Success:  2017-01-05  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-05\n",
      "Success:  2017-01-06  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-06\n",
      "Success:  2017-01-07  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-07\n",
      "Success:  2017-01-08  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-08\n",
      "Success:  2017-01-09  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-09\n",
      "Success:  2017-01-10  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-10\n",
      "Success:  2017-01-11  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-11\n",
      "Success:  2017-01-12  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-12\n",
      "Success:  2017-01-13  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-13\n",
      "Success:  2017-01-14  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-14\n",
      "Success:  2017-01-15  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-15\n",
      "Success:  2017-01-16  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-16\n",
      "Success:  2017-01-17  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-17\n",
      "Success:  2017-01-18  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-18\n",
      "Success:  2017-01-19  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-19\n",
      "Success:  2017-01-20  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-20\n",
      "Success:  2017-01-21  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-21\n",
      "Success:  2017-01-22  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-22\n",
      "Success:  2017-01-23  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-23\n",
      "Success:  2017-01-24  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-24\n",
      "Success:  2017-01-25  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-25\n",
      "Success:  2017-01-26  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-26\n",
      "Success:  2017-01-27  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-27\n",
      "Success:  2017-01-28  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-28\n",
      "Success:  2017-01-29  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-29\n",
      "Success:  2017-01-30  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-30\n",
      "Success:  2017-01-31  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-01-31\n",
      "Success:  2017-02-01  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-01\n",
      "Success:  2017-02-02  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-02\n",
      "Success:  2017-02-03  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-03\n",
      "Success:  2017-02-04  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-04\n",
      "Success:  2017-02-05  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-05\n",
      "Success:  2017-02-06  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-06\n",
      "Success:  2017-02-07  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-07\n",
      "Success:  2017-02-08  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-08\n",
      "Success:  2017-02-09  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-09\n",
      "Success:  2017-02-10  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-10\n",
      "Success:  2017-02-11  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-11\n",
      "Success:  2017-02-12  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-12\n",
      "Success:  2017-02-13  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-13\n",
      "Success:  2017-02-14  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-14\n",
      "Success:  2017-02-15  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-15\n",
      "Success:  2017-02-16  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-16\n",
      "Success:  2017-02-17  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-17\n",
      "Success:  2017-02-18  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-18\n",
      "Success:  2017-02-19  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-19\n",
      "Success:  2017-02-20  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-20\n",
      "Success:  2017-02-21  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-21\n",
      "Success:  2017-02-22  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-22\n",
      "Success:  2017-02-23  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-23\n",
      "Success:  2017-02-24  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-24\n",
      "Success:  2017-02-25  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-25\n",
      "Success:  2017-02-26  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-26\n",
      "Success:  2017-02-27  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-27\n",
      "Success:  2017-02-28  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-02-28\n",
      "Success:  2017-03-01  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-01\n",
      "Success:  2017-03-02  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-02\n",
      "Success:  2017-03-03  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-03\n",
      "Success:  2017-03-04  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-04\n",
      "Success:  2017-03-05  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-05\n",
      "Success:  2017-03-06  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-06\n",
      "Success:  2017-03-07  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-07\n",
      "Success:  2017-03-08  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-08\n",
      "Success:  2017-03-09  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-09\n",
      "Success:  2017-03-10  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-10\n",
      "Success:  2017-03-11  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-11\n",
      "Success:  2017-03-12  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-12\n",
      "Success:  2017-03-13  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-13\n",
      "Success:  2017-03-14  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-14\n",
      "Success:  2017-03-15  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-15\n",
      "Success:  2017-03-16  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-16\n",
      "Success:  2017-03-17  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-17\n",
      "Success:  2017-03-18  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-18\n",
      "Success:  2017-03-19  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-19\n",
      "Success:  2017-03-20  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-20\n",
      "Success:  2017-03-21  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-21\n",
      "Success:  2017-03-22  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-22\n",
      "Success:  2017-03-23  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-23\n",
      "Success:  2017-03-24  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success:  2017-03-25  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-25\n",
      "Success:  2017-03-26  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-26\n",
      "Success:  2017-03-27  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-27\n",
      "Success:  2017-03-28  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-28\n",
      "Success:  2017-03-29  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-29\n",
      "Success:  2017-03-30  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-30\n",
      "Success:  2017-03-31  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-03-31\n",
      "Success:  2017-04-01  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-01\n",
      "Success:  2017-04-02  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-02\n",
      "Success:  2017-04-03  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-03\n",
      "Success:  2017-04-04  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-04\n",
      "Success:  2017-04-05  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-05\n",
      "Success:  2017-04-06  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-06\n",
      "Success:  2017-04-07  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-07\n",
      "Success:  2017-04-08  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-08\n",
      "Success:  2017-04-09  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-09\n",
      "Success:  2017-04-10  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-10\n",
      "Success:  2017-04-11  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-11\n",
      "Success:  2017-04-12  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-12\n",
      "Success:  2017-04-13  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-13\n",
      "Success:  2017-04-14  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-14\n",
      "Success:  2017-04-15  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-15\n",
      "Success:  2017-04-16  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-16\n",
      "Success:  2017-04-17  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-17\n",
      "Success:  2017-04-18  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-18\n",
      "Success:  2017-04-19  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-19\n",
      "Success:  2017-04-20  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-20\n",
      "Success:  2017-04-21  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-21\n",
      "Success:  2017-04-22  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-22\n",
      "Success:  2017-04-23  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-23\n",
      "Success:  2017-04-24  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-24\n",
      "Success:  2017-04-25  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-25\n",
      "Success:  2017-04-26  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-26\n",
      "Success:  2017-04-27  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-27\n",
      "Success:  2017-04-28  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-28\n",
      "Success:  2017-04-29  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-29\n",
      "Success:  2017-04-30  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-04-30\n",
      "Success:  2017-05-01  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-01\n",
      "Success:  2017-05-02  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-02\n",
      "Success:  2017-05-03  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-03\n",
      "Success:  2017-05-04  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-04\n",
      "Success:  2017-05-05  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-05\n",
      "Success:  2017-05-06  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-06\n",
      "Success:  2017-05-07  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-07\n",
      "Success:  2017-05-08  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-08\n",
      "Success:  2017-05-09  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-09\n",
      "Success:  2017-05-10  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-10\n",
      "Success:  2017-05-11  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-11\n",
      "Success:  2017-05-12  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-12\n",
      "Success:  2017-05-13  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-13\n",
      "Success:  2017-05-14  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-14\n",
      "Success:  2017-05-15  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-15\n",
      "Success:  2017-05-16  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-16\n",
      "Success:  2017-05-17  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-17\n",
      "Success:  2017-05-18  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-18\n",
      "Success:  2017-05-19  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-19\n",
      "Success:  2017-05-20  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-20\n",
      "Success:  2017-05-21  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-21\n",
      "Success:  2017-05-22  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-22\n",
      "Success:  2017-05-23  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-23\n",
      "Success:  2017-05-24  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-24\n",
      "Success:  2017-05-25  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-25\n",
      "Success:  2017-05-26  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-26\n",
      "Success:  2017-05-27  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-27\n",
      "Success:  2017-05-28  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-28\n",
      "Success:  2017-05-29  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-29\n",
      "ERROR  :  attempt  1  ;  HTTP Error 404: Not Found\n",
      "          2017-05-30  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-30\n",
      "ERROR  :  attempt  2  ;  HTTP Error 404: Not Found\n",
      "          2017-05-30  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-30\n",
      "ERROR  :  attempt  3  ;  HTTP Error 404: Not Found\n",
      "          2017-05-30  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-30\n",
      "ERROR  :  attempt  -1  ;  SKIPPED\n",
      "          2017-05-30  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-30\n",
      "ERROR  :  attempt  1  ;  HTTP Error 404: Not Found\n",
      "          2017-05-31  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-31\n",
      "ERROR  :  attempt  2  ;  HTTP Error 404: Not Found\n",
      "          2017-05-31  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-31\n",
      "ERROR  :  attempt  3  ;  HTTP Error 404: Not Found\n",
      "          2017-05-31  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-31\n",
      "ERROR  :  attempt  -1  ;  SKIPPED\n",
      "          2017-05-31  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-05-31\n",
      "Success:  2017-06-01  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-01\n",
      "ERROR  :  attempt  1  ;  HTTP Error 404: Not Found\n",
      "          2017-06-02  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-02\n",
      "ERROR  :  attempt  2  ;  HTTP Error 404: Not Found\n",
      "          2017-06-02  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-02\n",
      "ERROR  :  attempt  3  ;  HTTP Error 404: Not Found\n",
      "          2017-06-02  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-02\n",
      "ERROR  :  attempt  -1  ;  SKIPPED\n",
      "          2017-06-02  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success:  2017-06-03  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-03\n",
      "Success:  2017-06-04  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-04\n",
      "Success:  2017-06-05  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-05\n",
      "Success:  2017-06-06  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-06\n",
      "Success:  2017-06-07  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-07\n",
      "Success:  2017-06-08  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-08\n",
      "Success:  2017-06-09  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-09\n",
      "Success:  2017-06-10  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-10\n",
      "Success:  2017-06-11  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-11\n",
      "Success:  2017-06-12  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-12\n",
      "Success:  2017-06-13  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-13\n",
      "Success:  2017-06-14  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-14\n",
      "Success:  2017-06-15  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-15\n",
      "Success:  2017-06-16  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-16\n",
      "Success:  2017-06-17  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-17\n",
      "Success:  2017-06-18  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-18\n",
      "Success:  2017-06-19  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-19\n",
      "Success:  2017-06-20  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-20\n",
      "Success:  2017-06-21  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-21\n",
      "Success:  2017-06-22  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-22\n",
      "Success:  2017-06-23  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-23\n",
      "Success:  2017-06-24  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-24\n",
      "Success:  2017-06-25  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-25\n",
      "Success:  2017-06-26  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-26\n",
      "Success:  2017-06-27  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-27\n",
      "Success:  2017-06-28  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-28\n",
      "Success:  2017-06-29  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-29\n",
      "Success:  2017-06-30  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-06-30\n",
      "Success:  2017-07-01  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-07-01\n",
      "Success:  2017-07-02  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-07-02\n",
      "Success:  2017-07-03  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-07-03\n",
      "Success:  2017-07-04  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-07-04\n",
      "Success:  2017-07-05  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-07-05\n",
      "Success:  2017-07-06  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-07-06\n",
      "Success:  2017-07-07  -  us  -  Top200  -  https://spotifycharts.com/regional/us/daily/2017-07-07\n"
     ]
    }
   ],
   "source": [
    "# WEB CRAWL CALL\n",
    "\n",
    "# date_var1 = datetime.date(2017,1,1)\n",
    "# date_var2 = datetime.date(2017,1,5)\n",
    "# crawl_df = crawl_spotify_charts(\n",
    "#     start_date=date_var1,\n",
    "#     end_date=date_var2,\n",
    "#     select_regions_dict={'us':'United States'}, \n",
    "#     crawl_viral=True)\n",
    "\n",
    "crawl_df = crawl_spotify_charts(\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    select_regions_dict={'us':'United States'}, \n",
    "    crawl_viral=True)\n",
    "\n",
    "crawl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Test Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oneday_delta= datetime.timedelta(days = 1)\n",
    "\n",
    "# jan2017_strt = datetime.date(2017,1,1)\n",
    "# jan2017_5 = datetime.date(2017,1,5)\n",
    "# feb2017_strt = datetime.date(2017,2,1)\n",
    "# mar2017_strt = datetime.date(2017,3,1)\n",
    "\n",
    "# jan2017_end = feb2017_strt - oneday_delta\n",
    "# feb2017_end = mar2017_strt - oneday_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Test Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Test Call:\n",
    "# crawl_spotify_charts(start_date = jan2017_strt, \n",
    "#                      end_date = mar2017_strt,\n",
    "#                      select_regions_dict={'us':'United States'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
