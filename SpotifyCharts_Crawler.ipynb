{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TO DO:\n",
    "\n",
    "1. **File naming:**\n",
    "    1. ~~Rename 'regional' chart_type to 'top200'~~\n",
    "    2. ~~Rename 'viral' chart_type to 'viral50'~~\n",
    "    3. ~~Rename country_abbrv dirs to country_name dirs~~\n",
    "        1. ~~csv of country_abbrvs to country_names~~\n",
    "    4. Create different enable_levels of CSV creation\n",
    "        1. per day (*enabled*)\n",
    "        2. per month\n",
    "        3. per year (*enabled*)\n",
    "        4. per country\n",
    "        5. per pull (*enabled*)\n",
    "    5. Abstraction of make-dir methods (creates directory $\\rightarrow$ returns directory path)\n",
    "2. ~~**Enable chart skipping:** (*maybe for each region page, scrape available dates from dropdown*)~~\n",
    "    1. ~~Chart non-existant for region~~\n",
    "    2. ~~ate non-existant for region~~\n",
    "    3. ~~*Maybe:*~~\n",
    "        1. ~~spotify charts crawl; passed(crawl_params)~~\n",
    "        2. ~~crawl chart type; passed(chart_types, chart_regions)~~\n",
    "        3. ~~crawl chart region; passed(chart_dates)~~\n",
    "        4. ~~*n.b. create dataframe on lowest level crawl; i.e. individual chart crawl, passing resulting dataframe all the way back to initial call, allowing empty dataframes to be appended for non-existant charts~~\n",
    "    4. ~~*Or Maybe:*~~\n",
    "       1. ~~if chart crawl fails, append URL to a list~~\n",
    "       2. ~~after web crawl, double check each fail~~\n",
    "    5. ~~*Or Or Maybe:*~~\n",
    "        1. ~~handle page non existant HTTPerrors differently in try...expect block~~\n",
    "        2. ~~i.e. handles IncompleteRead errors by:~~\n",
    "            1. ~~append to list of failed crawls, retrying crawls at the end~~\n",
    "            2. ~~or simply retrying the url (*once?; check exception_logs, but it seems IncompleteRead errors are resolved on a single retry, i.e. only fail on attempt = 0, not attempt >= 1)~~\n",
    "    6. ~~***Determine start/end dates of entire pull from oldest/newest dropdown dates*** ~~\n",
    "3. **Create data summary methods:**\n",
    "    1. 200 entries for each Top200 chart\n",
    "    2. 50 entries for each Viral50 chart\n",
    "    3. Per country, per year, per month, per day (i.e. highlighting day_charts that are not uniform)\n",
    "4. **Create data extraction methods:**\n",
    "    1. List of unique songs; could use???:\n",
    "        1. `track_title` $\\Leftarrow\\Rightarrow$ `track_title`\n",
    "            1. (pro) *do not change*\n",
    "            2. (con) *naming may not be unifromly formatted across sites*\n",
    "            3. (con) *many songs have the same name*\n",
    "            4. ***maybe:***\n",
    "                1. *use string compare methods that disregard formatting*\n",
    "                2. *compare strings with different formatting combinations*\n",
    "                3. *accept some threshold of string similarity*\n",
    "                4. *also compare `track_artist` using the same string compare methods*\n",
    "        2. Spotify URL\n",
    "            1. (pro) *same songs may have same spotify url*\n",
    "            2. (con) *urls can change overtime*\n",
    "        4. validate using a combination of both\n",
    "5. **Determine database design:**\n",
    "    1. create a UML diagram\n",
    "    2. consider information:\n",
    "        1. Tables\n",
    "        2. Primary Keys\n",
    "        3. Foriegn Keys\n",
    "    3. determine all information needed\n",
    "6. **Adapt file saving to GoogleDrive:**\n",
    "    1. extract directory creation and file saving\n",
    "    2. determine neccessary libraries\n",
    "    3. create GoogleDrive saving methods\n",
    "7. **Write signatures/documentations/annotations for code:**\n",
    "    1. import reasoning\n",
    "    2. program description\n",
    "    3. method statements\n",
    "8. ~~**Create master execption log:**~~\n",
    "    1. ~~include chart_type, chart_region in exception logging~~\n",
    "    2. ~~append all chart_scrape logs together~~\n",
    "9. **Repeated code abstraction:**\n",
    "    1. making directories\n",
    "    2. making file path names\n",
    "    3. ~~making soup~~\n",
    "    4. saving to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import http.client\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import dateutil.parser\n",
    "import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_trend_type(trend_fill:str) -> str:\n",
    "    \"\"\"Returns the trend type given the fill color of the Spotify Chart trend SVG.\"\"\"\n",
    "    # i. color: green; shape: up-triangle\n",
    "    if trend_fill == '#84bd00':\n",
    "        return 'Up'\n",
    "    # ii. color: gray; shape: horizontal-rectangle\n",
    "    elif trend_fill == '#3e3e40':\n",
    "        return 'Flat'\n",
    "    # iii. color: red; shape: down-triangle\n",
    "    elif trend_fill == '#bd3200':\n",
    "        return 'Down'\n",
    "    # iv. color: blue; shape: circle\n",
    "    elif trend_fill == '#4687d7':\n",
    "        return 'New'\n",
    "    # v. unhandled: non-v.1.1 tend_fill\n",
    "    else:\n",
    "        return 'UNKNOWN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datestamp_str(date):\n",
    "    datestamp_str = date.strftime('%Y-%m-%d')\n",
    "    return datestamp_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp_str():\n",
    "    now_datetime = datetime.datetime.now()\n",
    "    timestamp_str = now_datetime.strftime('D-%Y-%m-%d_T-%H-%M-%S-%f')\n",
    "    return timestamp_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    \"\"\"Returns the BeautifulSoup object for the given URL\"\"\"\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'\n",
    "    url_req = urllib.request.Request(url, headers = {'User-Agent': user_agent})\n",
    "    \n",
    "    req_resp = urllib.request.urlopen(url_req)\n",
    "    req_read = req_resp.read()\n",
    "    req_resp.close()\n",
    "    \n",
    "    return BeautifulSoup(req_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_region_dict(chart_type_url):\n",
    "\n",
    "    chart_soup = get_soup(chart_type_url)\n",
    "    region_li_tags = chart_soup.find('div', \n",
    "                                     attrs={'class':'responsive-select','data-type':'country'}).find_all('li')\n",
    "    \n",
    "    region_dict = {}\n",
    "    for li_tag in region_li_tags:\n",
    "        region_abrv = li_tag.get('data-value').strip()\n",
    "        region_name = li_tag.get_text()\n",
    "        region_dict[region_abrv] = region_name\n",
    "    \n",
    "    return region_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_chart(date_soup, chart_name, chart_region, chart_date):\n",
    "    \"\"\"Returns pd/DataFrame of data scraped from each chart's <table class=\"chart-table\" ...> tag\"\"\"\n",
    "    tr_tags = date_soup.find('table', class_='chart-table').find('tbody').find_all('tr')\n",
    "    \n",
    "    chart_data = []\n",
    "    for tr_tag in tr_tags:\n",
    "        \n",
    "        song_info = []\n",
    "        \n",
    "        # 0. Chart info\n",
    "        song_info.append(chart_name)\n",
    "        song_info.append(chart_region)\n",
    "        song_info.append(chart_date)\n",
    "        \n",
    "        # 1. Chart Position; element of {1, 2, ... , 200}\n",
    "        pos_tag = tr_tag.find('td', class_='chart-table-position')\n",
    "        pos_val = int(pos_tag.get_text().strip())\n",
    "        song_info.append(pos_val)\n",
    "        \n",
    "        # 2. Streaming Trend Type; element of {Up, Flat, Down, New}\n",
    "        trend_tag = tr_tag.find('td', class_='chart-table-trend')\n",
    "        trend_fill = trend_tag.find('svg').get('fill').strip()\n",
    "        trend_val = det_trend_type(trend_fill)\n",
    "        song_info.append(trend_val)\n",
    "        \n",
    "        # 3. Track Title and Artist\n",
    "        track_tag = tr_tag.find('td', class_='chart-table-track')\n",
    "        title_val = track_tag.find('strong').get_text().strip()\n",
    "        artist_val = track_tag.find('span').get_text().strip().replace('by ', '')\n",
    "        song_info.append(title_val)\n",
    "        song_info.append(artist_val)\n",
    "        \n",
    "        # 4. Total Streams\n",
    "        if chart_name == 'Viral50':\n",
    "            song_info.append(None)\n",
    "        else:\n",
    "            streams_tag = tr_tag.find('td', class_='chart-table-streams')\n",
    "            streams_val = int(streams_tag.get_text().strip().replace(',',''))\n",
    "            song_info.append(streams_val)\n",
    "        \n",
    "        # 5. Icon and Spotify URLs\n",
    "        icon_tag = tr_tag.find('td', class_='chart-table-image')\n",
    "        icon_url_val = icon_tag.find('img').get('src').strip()\n",
    "        spotify_url_val = icon_tag.find('a').get('href').strip()\n",
    "        song_info.append(icon_url_val)\n",
    "        song_info.append(spotify_url_val)\n",
    "        \n",
    "        chart_data.append(song_info)\n",
    "\n",
    "    return chart_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_charts_dates(region_url, start_date, end_date):\n",
    "    \n",
    "    region_soup = get_soup(region_url)\n",
    "    date_div = region_soup.find('div', attrs={'class':'responsive-select','data-type':'date'})\n",
    "    \n",
    "    if date_div is not None:\n",
    "        return []\n",
    "    else:\n",
    "        date_li_tags = date_div.find_all('li')\n",
    "        region_chart_dates_list = []\n",
    "        \n",
    "        for li_tag in date_li_tags:\n",
    "            date = dateutil.parser.isoparse(li_tag.get('data-value').strip()).date()\n",
    "            \n",
    "            if date >= start_date and date <= end_date:\n",
    "                region_chart_dates_list.append(date)\n",
    "                \n",
    "        return region_chart_dates_list.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-bd14c69efb5e>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-bd14c69efb5e>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    crawl_top = False: bool,\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def crawl_spotify_charts(start_date: datetime.date,\n",
    "                         end_date: datetime.date,\n",
    "                         crawl_top = False: bool,\n",
    "                         crawl_viral = False: bool,\n",
    "                         colab_save = False: bool) -> pd.DataFrame:\n",
    "    \"\"\"Returns a pd.DataFrame containing information scraped from https://spotifycharts.com.\"\"\"\n",
    "    # 1. loop through charts\n",
    "    BASE_URL = 'https://spotifycharts.com'\n",
    "    chart_type_dict = {'regional':'Top200', 'viral':'Viral50'}\n",
    "    chart_interval_list = ['daily', 'weekly']\n",
    "    \n",
    "    # 2. instantiates result lists\n",
    "    result_data = []\n",
    "    error_list = []\n",
    "    \n",
    "    # 3. scrapes all available charts\n",
    "    # 3. loops through chart types\n",
    "    for chart_type, chart_name in chart_dict.items():\n",
    "        \n",
    "        chart_type_url = BASE_URL + '/' + chart_type\n",
    "        chart_regions_dict = scrape_region_dict(chart_type_url)\n",
    "            \n",
    "        # 3.2. loops through chart regions\n",
    "        for chart_region_abrv, chart_region_name in chart_regions_dict.items():\n",
    "            \n",
    "            # 3.3. loops through chart intervals\n",
    "            for chart_interval in chart_intervals_list:\n",
    "                \n",
    "                latest_charts_url = chart_type_url + '/' + chart_region_abrv + '/' + chart_interval\n",
    "                \n",
    "                try:\n",
    "                    charts_dates_list = scrape_charts_dates(chart_region_url, start_date, end_date)\n",
    "                \n",
    "\n",
    "                # 3.4. loops through all available chart dates\n",
    "                for chart_date in charts_dates_list:\n",
    "\n",
    "                    chart_date_url = chart_region_url + '/' + chart_date.isoformat()\n",
    "\n",
    "                    # 3.4.1. tries, then retries 3 times, to read the chart's html page\n",
    "                    for i in range(0,3):\n",
    "                        # 3.4.1.1. tries to read the url, and if successful, scrapes and appends its data\n",
    "                        try:\n",
    "                            chart_date_soup = get_soup(chart_date_url)\n",
    "                            chart_date_data = scrape_chart(chart_date_soup, \n",
    "                                                           chart_name, \n",
    "                                                           chart_region_name, \n",
    "                                                           chart_date)\n",
    "                            result_data.append(chart_date_data)\n",
    "                            print('Appened: ', chart_date, ' - ', chart_region_abrv, ' - ', chart_type)\n",
    "                            break\n",
    "\n",
    "                        # 3.4.1.2. cathces IncompleteRead exceptions, retrying the URL twice\n",
    "                        except http.client.IncompleteRead as e:\n",
    "                            print('Error: ', chart_date, ' - ', chart_region_abrv, ' - ', chart_type, ', Attempt ' , i ,'\\n\\turl: ', chart_date_url, ' ; ', sys.exc_info()[0], '\\n\\n')\n",
    "                            error_list.append([chart_type, chart_region_name, chart_date, chart_date_url, i, e])\n",
    "                            continue\n",
    "                    \n",
    "                    # 3.5. otherwise, if the loop falls through, skips this chart date\n",
    "                    else:\n",
    "                        print('Error: ', chart_date, ' - ', chart_region_abrv, ' - ', chart_type, ', Attempt ' , i ,'\\n\\turl: ', chart_date_url, ' ; ', sys.exc_info()[0], '\\n\\n')\n",
    "                        error_list.append([chart_type, chart_region_name, chart_date, chart_date_url, i, 'skipped'])\n",
    "                        continue\n",
    "\n",
    "    # 4. creates both the result and error pd.DataFrames from the accumulated list of data entries\n",
    "    result_col_names = ['Chart', 'Region' , 'Date', 'Position', 'Trend', \n",
    "                        'Title', 'Artist', 'Streams', 'Icon_URL', 'Spotify_URL']\n",
    "    result_df = pd.DataFrame(result_data, columns = result_col_names)\n",
    "    \n",
    "    # 5. saves both the result and error pd.DataFrame as csv files\n",
    "    timestamp_str = get_timestamp_str()\n",
    "    tgt_dir_path = './pull_' + timestamp_str\n",
    "    if not os.path.exists(tgt_dir_path): os.mkdir(tgt_dir_path)\n",
    "    \n",
    "    now_datetime_str = get_timestamp_str()\n",
    "    start_date_str = get_datestamp_str(start_date)\n",
    "    end_date_str = get_datestamp_str(end_date)\n",
    "\n",
    "    result_filename = 'Result_' + now_datetime_str + '_FrmD-' + start_date_str + '_ToD-' + end_date_str + '.csv'\n",
    "    result_filepath = os.path.join(tgt_dir_path, result_filename)\n",
    "    result_df.to_csv(result_filepath)\n",
    "    \n",
    "    # 6. returns the result pd.DataFrame\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEB CRAWL CALL\n",
    "# crawl_spotify_charts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. vars:\n",
    "oneday_delta= datetime.timedelta(days = 1)\n",
    "\n",
    "jan2017_strt = datetime.date(2017,1,1)\n",
    "feb2017_strt = datetime.date(2017,2,1)\n",
    "mar2017_strt = datetime.date(2017,3,1)\n",
    "\n",
    "jan2017_end = feb2017_strt - oneday_delta\n",
    "feb2017_end = mar2017_strt - oneday_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. test call:\n",
    "test_df = crawl_spotify_charts(start_date = jan2017_end, end_date = feb2017_strt)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = '2019-10-10'\n",
    "date_split = date_str.split('-')\n",
    "datetime.date(int(date_split[0]), int(date_split[1]), int(date_split[2])).isoformat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateutil.parser.isoparse(date_str).date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### [ $ \\uparrow $ ]  Implemented Code\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### [ $ \\downarrow $ ]  Scrapped Code\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client_id = '987f8aab8f804962a2f19a86e310905c'\n",
    "# client_secret = 'bdb457608ae84339ad7d3c41696cf10e'\n",
    "\n",
    "# client_credentials_manager = spotipy.oauth2.SpotifyClientCredentials(client_id, client_secret)\n",
    "# sp = spotipy.Spotify(client_credentials_manager = client_credentials_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNUSED: while the downloadable csv does contain most of the target information, it proved...\n",
    "# ... difficult to parse unreliable in formatting\n",
    "\n",
    "# scrape_chart_csv : str -> DataFrame\n",
    "# returns a DataFrame of information scraped from the csv-styled downloadable byte file \n",
    "def scrape_chart_csv(csv_url):\n",
    "    \n",
    "    # i. handles the URL\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'\n",
    "    url_req = urllib.request.Request(csv_url, headers = {'User-Agent': user_agent})\n",
    "    url_resp = urllib.request.urlopen(url_req)\n",
    "    \n",
    "    # ii. handles the opened byte file\n",
    "    csv_bytes = url_resp.read()\n",
    "    csv_str = csv_bytes.decode(encoding='utf-8', errors='strict')\n",
    "    \n",
    "    csv_data_str = csv_str[csv_str.find('Position'):].strip()\n",
    "    csv_data_lines = csv_data_str.split('\\n')\n",
    "    \n",
    "    url_resp.close()\n",
    "\n",
    "    # iii. loops through the data string list extracting needed information\n",
    "    data_list = []\n",
    "    for line in csv_data_lines:\n",
    "        \n",
    "        song_info = []\n",
    "        \n",
    "        # 1. Chart Position\n",
    "        position = line.partition(',')[0].strip()\n",
    "        if position.isdigit():\n",
    "            position = int(position)\n",
    "        song_info.append(position)\n",
    "            \n",
    "        # 2. Spotify URL\n",
    "        url = line.partition(',')[2].rpartition(',')[2].strip()\n",
    "        song_info.append(url)\n",
    "        \n",
    "        # 3. Total Streams\n",
    "        streams = line.partition(',')[2].rpartition(',')[0].rpartition(',')[2].strip()\n",
    "        if streams.isdigit():\n",
    "            streams = int(streams)\n",
    "        song_info.append(streams)\n",
    "        \n",
    "        # 4. Track Artist\n",
    "        artist = line.partition(',')[2].rpartition(',')[0].rpartition(',')[0].rpartition(',')[2].strip()\n",
    "        artist = artist.replace('\"','').strip()\n",
    "        song_info.append(artist)\n",
    "        \n",
    "        # 5. Track Title\n",
    "        title = line.partition(',')[2].rpartition(',')[0].rpartition(',')[0].rpartition(',')[0].strip()\n",
    "        title = title.replace('\"','').strip()\n",
    "        song_info.append(title)\n",
    "        \n",
    "        data_list.append(song_info)\n",
    "    \n",
    "    # iv. creates a DataFrame of the scraped song information\n",
    "    cols_names = data_list[0]\n",
    "    song_data = data_list[1:]\n",
    "    chart_csv_df = pd.DataFrame(song_data, columns = cols_names)\n",
    "    \n",
    "    return chart_csv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_chart(chart_soup):\n",
    " \n",
    "    top_charts_df = pd.DataFrame(columns = col_names)\n",
    "    \n",
    "    # ii. creates the base URL\n",
    "    base_url = 'https://spotifycharts.com/' + chart_type + '/' + chart_region + '/' + chart_interval\n",
    "\n",
    "    # iii. loops through DateTimes from start_date to end_date inclusively\n",
    "    try_attempt = 0\n",
    "    curr_date_iter = start_date\n",
    "    while curr_date_iter <= end_date:\n",
    "        \n",
    "        # 1. formats the date and creates its URL\n",
    "        curr_iso_date = curr_date_iter.isoformat()\n",
    "        date_url = base_url + '/' + curr_iso_date\n",
    "        \n",
    "        # 2. scrapes the URL into a DataFrame\n",
    "        try:\n",
    "            date_table_df = scrape_chart_table(date_url, chart_type, chart_region,  curr_date_iter, col_names)\n",
    "\n",
    "        except Exception  as e:\n",
    "            if try_attempt < 3:\n",
    "                print('Error: ', curr_iso_date, ', Attempt ' , try_attempt ,'\\n\\turl: ', date_url, ' ; ', sys.exc_info()[0], '\\n\\n')\n",
    "                try_attempt += 1\n",
    "                continue\n",
    "            else:\n",
    "                print('Error: ', curr_iso_date, ', Attempt ' , try_attempt ,'\\n\\turl: ', date_url, ' ; ', sys.exc_info()[0], '\\n\\n')\n",
    "                exception_list.append([curr_iso_date, curr_date_iter, date_url, str(e)])\n",
    "                try_attempt = 0\n",
    "                curr_date_iter = curr_date_iter + datetime.timedelta(days = 1)\n",
    "                continue\n",
    "        \n",
    "        # •••. creates the necessary directories and saves the files\n",
    "        if enable_date_csvs: save_date_csv(curr_date_iter, date_table_df, crawl_dir_name)\n",
    "        \n",
    "        # 3. appends the DataFrame to the running total dataset\n",
    "        top_charts_df = top_charts_df.append(date_table_df, ignore_index = True)\n",
    "        \n",
    "        # 4. advances the while-iterator\n",
    "        try_attempt = 0\n",
    "        curr_date_iter = curr_date_iter + datetime.timedelta(days = 1)\n",
    "    \n",
    "    # iv. formats the resulting DataFrame\n",
    "\n",
    "    # 1. specifies the name of the csv file\n",
    "    now_datetime_str = get_timestamp_str()\n",
    "    start_date_str = get_datestamp_str(start_date)\n",
    "    end_date_str = get_datestamp_str(end_date)\n",
    "\n",
    "    # 2. creates and saves to the file path top charts DataFrame as a csv file\n",
    "    chart_name =  chart_type.title() + '-' + chart_region.title() + '-'  + chart_interval.title() + '_' \n",
    "    all_charts_filename = chart_name + 'AllCharts_' + 'FrmD-' + start_date_str + '-ToD-' + end_date_str + '_At-' + now_datetime_str +  '.csv'\n",
    "    all_charts_filepath = os.path.join(crawl_dir_name, all_charts_filename)\n",
    "    top_charts_df.to_csv(all_charts_filepath)\n",
    "\n",
    "    # •••. instantiates exception log DataFrame\n",
    "    el_col_names = ['Atmpt_Date', 'Atmpt_datetime', 'Atmpt_URL', 'Exception']\n",
    "    exception_log_df = pd.DataFrame(data = exception_list, columns = el_col_names)\n",
    "\n",
    "    # •••. creates and saves to the file path exception log DataFrame as a csv file \n",
    "    exception_log_filename = 'ExceptionLog_' + now_datetime_str + '_FrmD-' + start_date_str + '_ToD-' + end_date_str + '.csv'\n",
    "    exception_log_filepath = os.path.join(crawl_dir_name, exception_log_filename)\n",
    "    exception_log_df.to_csv(exception_log_filepath)\n",
    "    \n",
    "    # vi. returns: top charts DataFrame for the date range\n",
    "    return top_charts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "#\n",
    "def save_date_csv(curr_date, date_table_df, crawl_dir_name):\n",
    "    \n",
    "    year_dir_name = curr_date.strftime('%Y')\n",
    "    year_dir_path = os.path.join(crawl_dir_name, year_dir_name)\n",
    "    if not os.path.exists(year_dir_path): os.mkdir(year_dir_path)\n",
    "    \n",
    "    month_dir_name = curr_date.strftime('%m')\n",
    "    month_dir_path = os.path.join(year_dir_path, month_dir_name)\n",
    "    if not os.path.exists(month_dir_path): os.mkdir(month_dir_path)\n",
    "    \n",
    "    timestamp_str = get_timestamp_str()\n",
    "    curr_datestamp = get_datestamp_str(curr_date)\n",
    "    \n",
    "    curr_date_filename = 'ChartDate-' + curr_datestamp + '_At-' + timestamp_str + '.csv'\n",
    "    curr_date_filepath = os.path.join(month_dir_path, curr_date_filename)\n",
    "    \n",
    "    date_table_df.to_csv(curr_date_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
